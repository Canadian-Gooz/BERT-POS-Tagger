{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db61453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9019e02",
   "metadata": {},
   "outputs": [],
   "source": [
    " tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae718d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_ds(file):\n",
    "    inp = ['']\n",
    "    targ = [[]]\n",
    "    idx = 0\n",
    "    for line in file:\n",
    "        try:\n",
    "            word, tag, _ = line.split(' ')\n",
    "            inp[idx]+= word + ' '\n",
    "            targ[idx].append(tag)\n",
    "        except:\n",
    "#             tf.convert_to_tensor(inp[idx])\n",
    "            targ[idx]=np.asarray(targ[idx])\n",
    "            idx +=1\n",
    "            inp.append('')\n",
    "            targ.append([])\n",
    "    return tf.convert_to_tensor(inp), targ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0311bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.txt\",mode='r', encoding = \"utf-8\") as f:\n",
    "    X_train, y_train = prep_ds(f)\n",
    "with open(\"test.txt\",mode='r', encoding = \"utf-8\") as f:\n",
    "    X_test, y_test = prep_ds(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74900e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.ragged.constant(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd220bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(len,[x.numpy().split() for x in X_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b3a4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"Confidence in the pound is widely expected to take another sharp dive if trade figures for September , due for release tomorrow , fail to show a substantial improvement from July and August 's near-record deficits . \"\n",
      " b\"Chancellor of the Exchequer Nigel Lawson 's restated commitment to a firm monetary policy has helped to prevent a freefall in sterling over the past week . \"], shape=(2,), dtype=string)\n",
      "[['NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'IN', 'NNP', ',', 'JJ', 'IN', 'NN', 'NN', ',', 'VB', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'POS', 'JJ', 'NNS', '.'], ['NNP', 'IN', 'DT', 'NNP', 'NNP', 'NNP', 'POS', 'VBN', 'NN', 'TO', 'DT', 'NN', 'JJ', 'NN', 'VBZ', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:2])\n",
    "print(y_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac259d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TagVectorization:\n",
    "    def __init__(self,tag_corpus,seq_length):\n",
    "        self.tags = list(set([tag for tags in tag_corpus for tag in tags]))\n",
    "        self.sparse_vect = [i for i in range(1,len(self.tags)+1)]\n",
    "        self.tag_to_vec = {self.tags[i]:self.sparse_vect[i] for i in range(len(self.tags))}\n",
    "        self.vec_to_tag = {self.sparse_vect[i]:self.tags[i] for i in range(len(self.tags))}\n",
    "        self.seq_length = seq_length\n",
    "    def tag2vec(self, tags):\n",
    "        vectors = []\n",
    "        for sample in tags.numpy():\n",
    "            vecs = [self.tag_to_vec[tag.decode('utf-8')] for tag in sample]\n",
    "            vecs = tf.convert_to_tensor(vecs)\n",
    "            vecs = tf.pad(vecs,tf.constant([[1,1]]))\n",
    "            padded_vecs = tf.concat([vecs,tf.zeros(self.seq_length-len(vecs),dtype=tf.dtypes.int32)],0)\n",
    "            vectors.append(padded_vecs)\n",
    "        return tf.convert_to_tensor(vectors)\n",
    "    def vec2tag(self, vecs, mask):\n",
    "        mask = tf.cast(mask,dtype=tf.bool)\n",
    "        no_pad = tf.ragged.boolean_mask(vecs, mask)[:,1:-1].numpy().tolist()\n",
    "        return [list(map(self.vec_to_tag.get,sentence)) for sentence in no_pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31a7e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = TagVectorization(y_train,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6022029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_url =  'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "bert_url = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
    "bert_urls = [preproc_url,bert_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35f417a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self,bert_preproc, bert_model, seq_length):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        self.preproc =  hub.load(bert_preproc)\n",
    "        self.bert = hub.KerasLayer(bert_model,trainable= True)\n",
    "        self.seq_length = seq_length\n",
    "    def call(self, text):\n",
    "        \n",
    "        tokenize = hub.KerasLayer(self.preproc.tokenize)\n",
    "        tokens = tokenize(text)\n",
    "        bert_pack_inputs = hub.KerasLayer(\n",
    "            self.preproc.bert_pack_inputs,\n",
    "            arguments=dict(seq_length=self.seq_length))\n",
    "        encoder_inputs = bert_pack_inputs([tokens])\n",
    "        mask_idx = encoder_inputs['input_mask']\n",
    "        \n",
    "        outputs = self.bert(encoder_inputs)['sequence_output']\n",
    "#         masked_output = []\n",
    "#         for i in range(outputs.shape[1]):\n",
    "#             if  mask_idx[0][i]==1:\n",
    "#                 masked_output.append(outputs[0][i])\n",
    "#         x = tf.convert_to_tensor([masked_output])      \n",
    "        return outputs, mask_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a37e21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(tf.keras.layers.Layer):\n",
    "    def __init__(self, units,n_tags):\n",
    "        super(Tagger, self).__init__()\n",
    "        self.lstm_1 = tf.keras.layers.LSTM(units,return_sequences=True,return_state=True)\n",
    "        self.lstm_2 = tf.keras.layers.LSTM(n_tags,return_sequences=True)\n",
    "        self.W1 = tf.keras.layers.Dense(n_tags)\n",
    "    def call(self,inputs):\n",
    "        output_seq, mem_state, state = self.lstm_1(inputs)\n",
    "        mem_state = self.W1(mem_state)\n",
    "        state = self.W1(state)\n",
    "        logits = self.lstm_2(output_seq,initial_state=[mem_state,state])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189c88d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosTagger(tf.keras.Model):\n",
    "    def __init__(self, tagger_units,n_tags,seq_length, bert_urls, tag_corpus):\n",
    "        super(PosTagger, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.tagger_units = tagger_units\n",
    "        self.n_tags = n_tags\n",
    "        self.bert_urls = bert_urls\n",
    "        \n",
    "        self.tagvec = TagVectorization(tag_corpus,seq_length)\n",
    "        self.bert = BertEmbeddings(bert_urls[0],bert_urls[1],seq_length)\n",
    "        self.tagger = Tagger(tagger_units,n_tags)\n",
    "    def train_step(self,inputs):\n",
    "        text, tags = inputs\n",
    "        \n",
    "        targets = self.tagvec.tag2vec(tags)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            embeddings, mask = self.bert(text)\n",
    "            logits = self.tagger(embeddings)\n",
    "            loss = self.loss(targets,logits)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {'loss': loss}\n",
    "    def call(self,inputs):\n",
    "        embeddings, mask = self.bert(inputs)\n",
    "        logits = self.tagger(embeddings)\n",
    "        \n",
    "        predictions = tf.argmax(logits,axis=-1)\n",
    "        tags = self.tagvec.vec2tag(predictions,mask)\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4bbbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PosTagger(256,45,100,bert_urls,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53e6fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=MaskedLoss(), optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d9b2b3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 29s 249ms/step - loss: 748.1553\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 726.7146\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 714.5664\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 702.3272\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 697.1490\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 685.6608\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 672.7261\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 662.2331\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 28s 276ms/step - loss: 652.5458\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 645.0353\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 634.1220\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 628.0810\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 620.2925\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 614.9883\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 608.0309\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 604.9191\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 599.1755\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 595.4561\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 591.2154\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 588.9705\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 29s 287ms/step - loss: 585.5065\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 584.9540\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 39s 386ms/step - loss: 581.5763\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 11s 104ms/step - loss: 578.2556\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 7s 71ms/step - loss: 578.0891\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 575.0133\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 573.2922\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 570.6614\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 569.3670\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 567.7748\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 564.6097\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 564.3234\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 562.1747\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 562.8253\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 560.4808\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 560.0066\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 557.3289\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 557.3393\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 558.4404\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 557.0397\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 554.8677\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 556.0998\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 557.0000\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 555.3779\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 553.9972\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 554.0182\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 554.8507\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 7s 67ms/step - loss: 553.9992\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 7s 68ms/step - loss: 553.5413\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 7s 66ms/step - loss: 552.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28a5658edc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train[:1000],y=y[:1000],batch_size=10, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30cb2b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "        loss *= mask\n",
    "\n",
    "        return tf.reduce_sum(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3060b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
